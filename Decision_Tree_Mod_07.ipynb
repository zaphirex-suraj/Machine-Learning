{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment on Decision Tree || Module-07\n",
        "**Assignment Code:** DA-AG-012\n",
        "\n",
        "**Learner's Name:** Suraj Vishwakarma  \n",
        "**Email:** vishsurajfor@gmail.com\n",
        "\n",
        "This notebook contains the solution of 10 questions from the assignment  and runnable Python code where applicable.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Q1: What is a Decision Tree, and how does it work in the context of classification?  \n",
        "\n",
        "**Answer:** -A **Decision Tree** is a powerful supervised machine learning algorithm that is widely used for solving **classification problems**. It is represented in the form of a tree-like model of decisions, where each internal node tests a feature, each branch corresponds to the outcome of that test, and each leaf node assigns a class label. Decision Trees mimic human decision-making and are therefore highly interpretable.  \n",
        "\n",
        "---\n",
        "\n",
        "### Working of a Decision Tree in Classification:  \n",
        "\n",
        "1. **Root Node:**  \n",
        "   - The process starts from the root node which represents the entire dataset.  \n",
        "   - The algorithm looks for the feature that best separates the classes.  \n",
        "\n",
        "2. **Splitting Criteria:**  \n",
        "   - A suitable attribute is chosen using impurity measures such as **Entropy**, **Information Gain**, **Gini Index**, or **Chi-Square**.  \n",
        "   - The attribute that produces the most homogeneous (pure) child nodes is selected.  \n",
        "\n",
        "3. **Recursive Partitioning:**  \n",
        "   - The dataset is divided into smaller subsets based on the selected attribute.  \n",
        "   - This process continues recursively for each subset, creating branches of the tree.  \n",
        "\n",
        "4. **Leaf Nodes (Terminal Nodes):**  \n",
        "   - The recursion stops when either:  \n",
        "     - All data points in a node belong to the same class, or  \n",
        "     - Further splitting does not add value.  \n",
        "   - At this stage, the node becomes a leaf and is labeled with the class outcome.  \n",
        "\n",
        "---\n",
        "\n",
        "### Illustrative Example:  \n",
        "Suppose we are predicting whether a student will **Pass** or **Fail** based on *Study Hours* and *Attendance*.  \n",
        "\n",
        "- **Root Node:** Check `Study Hours > 3`.  \n",
        "   - If **Yes → Predict: Pass**.  \n",
        "   - If **No → Check Attendance > 75%`.  \n",
        "     - If **Yes → Predict: Pass**.  \n",
        "     - If **No → Predict: Fail**.  \n",
        "\n",
        "This shows how the decision tree uses **step-by-step feature-based questions** to classify outcomes.  \n",
        "\n",
        "---\n",
        "\n",
        "### Advantages of Decision Trees in Classification:  \n",
        "- Easy to understand and interpret (resembles human reasoning).  \n",
        "- Handles both numerical and categorical data.  \n",
        "- No need for data normalization or scaling.  \n",
        "- Produces a clear **set of rules (if–else)** for prediction.  \n",
        "\n",
        "### Limitations:  \n",
        "- **Overfitting:** Trees can grow very deep and memorize training data.  \n",
        "- **Bias toward features with many values:** Attributes with multiple splits may dominate.  \n",
        "- Sensitive to small changes in data.  \n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion:  \n",
        "A Decision Tree in classification works by recursively splitting the dataset into homogeneous groups based on features, ultimately leading to a class label at the leaves. Its simplicity and interpretability make it suitable for applications like **loan approval, medical diagnosis, fraud detection, and student performance prediction**. However, without pruning or ensemble methods, decision trees can easily overfit the training data.  \n",
        "\n",
        "## Q2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?  \n",
        "\n",
        "**Answer:** -When building a Decision Tree, the algorithm must decide **which feature to split on at each step**. To make this choice, it relies on **impurity measures**, which quantify how mixed (impure) or pure a node is in terms of class labels. Two commonly used measures are **Gini Impurity** and **Entropy**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Gini Impurity  \n",
        "- **Definition:** Gini impurity measures the probability that a randomly chosen sample from a node will be incorrectly classified if it were randomly labeled according to the class distribution.  \n",
        "- **Formula:**  \n",
        "$$\n",
        "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$  \n",
        "Where:  \n",
        "- $C$ = number of classes  \n",
        "- $p_i$ = probability of class $i$ in the node  \n",
        "\n",
        "- **Interpretation:**  \n",
        "  - A Gini of **0** means the node is pure (all samples belong to one class).  \n",
        "  - Higher Gini means higher impurity.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Entropy  \n",
        "- **Definition:** Entropy is a concept from information theory that measures the level of uncertainty or randomness in the data.  \n",
        "- **Formula:**  \n",
        "$$\n",
        "Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$  \n",
        "\n",
        "- **Interpretation:**  \n",
        "  - Entropy = **0** → node is pure (all samples belong to one class).  \n",
        "  - Entropy is maximum when classes are equally mixed (e.g., 50%-50% for binary).  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Impact on Splits  \n",
        "- Decision Trees use these measures to evaluate **how “good” a split is**.  \n",
        "- The goal is to **reduce impurity** as much as possible after a split.  \n",
        "- The algorithm calculates the **weighted average impurity** of child nodes, and the split that yields the **lowest impurity (or highest Information Gain)** is chosen.  \n",
        "\n",
        "**Information Gain (based on Entropy):**  \n",
        "$$\n",
        "IG = Entropy(parent) - \\sum_{k} \\frac{N_k}{N} \\cdot Entropy(child_k)\n",
        "$$  \n",
        "\n",
        "**For Gini:**  \n",
        "The algorithm simply chooses the split with the **lowest Gini impurity** in child nodes.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Numerical Example  \n",
        "Suppose a dataset has 10 samples: **6 Positive** and **4 Negative**.  \n",
        "\n",
        "- **Entropy at parent node:**  \n",
        "$$\n",
        "Entropy = -\\Big(0.6 \\log_2 0.6 + 0.4 \\log_2 0.4 \\Big) \\approx 0.97\n",
        "$$  \n",
        "\n",
        "- **Gini at parent node:**  \n",
        "$$\n",
        "Gini = 1 - (0.6^2 + 0.4^2) = 0.48\n",
        "$$  \n",
        "\n",
        "If a split divides the data into subsets with higher purity (e.g., [4P,1N] and [2P,3N]), both Entropy and Gini will **decrease**, showing that the split improved classification.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Comparison of Gini vs Entropy  \n",
        "- Both measures usually give **similar results** in practice.  \n",
        "- **Gini Impurity:**  \n",
        "  - Faster to compute.  \n",
        "  - Used by CART (Classification and Regression Trees).  \n",
        "- **Entropy:**  \n",
        "  - Based on information theory.  \n",
        "  - Used by ID3 and C4.5 algorithms.  \n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "Both **Gini Impurity** and **Entropy** are key metrics for evaluating splits in Decision Trees. They ensure that each decision point reduces uncertainty and leads toward purer nodes. By selecting attributes that minimize impurity, the tree becomes more effective at classification. While their mathematical formulation differs, their overall impact on the tree’s structure is quite similar.  \n",
        "\n",
        "## Q3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.  \n",
        "\n",
        "**Answer:** -Decision Trees are powerful but prone to **overfitting**, especially when they grow too deep and capture noise in the data. To address this, pruning techniques are applied. **Pruning** refers to stopping or cutting back the growth of the tree so that it generalizes better on unseen data. There are two main types: **Pre-Pruning** and **Post-Pruning**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Pre-Pruning (Early Stopping)  \n",
        "- **Definition:** Pre-pruning halts the tree growth **before it becomes too complex**. Instead of allowing the tree to grow fully, we set stopping conditions during the building phase.  \n",
        "- **Common strategies:**  \n",
        "  - Maximum depth of the tree (`max_depth`).  \n",
        "  - Minimum number of samples required at a node (`min_samples_split`).  \n",
        "  - Minimum information gain required for a split.  \n",
        "- **Example:** If a node has fewer than 5 samples, the algorithm stops further splitting and makes it a leaf node.  \n",
        "\n",
        "**Practical Advantage:**  \n",
        "- **Efficiency in computation.** Since the tree stops growing early, it reduces training time and memory usage. Useful in real-time or large-scale applications.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Post-Pruning (Cost-Complexity Pruning)  \n",
        "- **Definition:** Post-pruning allows the decision tree to grow to its full depth and then **removes unnecessary branches** that do not improve accuracy significantly.  \n",
        "- **Methods:**  \n",
        "  - Reduced Error Pruning: Remove a branch if performance on a validation set does not decrease.  \n",
        "  - Cost-Complexity Pruning: Prunes subtrees that add little value to prediction while increasing complexity.  \n",
        "- **Example:** A fully grown tree might classify a few outlier samples incorrectly. Pruning those branches improves generalization on test data.  \n",
        "\n",
        "**Practical Advantage:**  \n",
        "- **Improved generalization.** By trimming overfitted parts of the tree, post-pruning reduces variance and increases predictive accuracy on unseen data.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Key Differences  \n",
        "\n",
        "| Aspect                | Pre-Pruning                       | Post-Pruning                          |\n",
        "|------------------------|-----------------------------------|---------------------------------------|\n",
        "| **When applied**       | During tree construction          | After a full tree is built             |\n",
        "| **Control**            | Stops splits based on conditions  | Cuts back unnecessary branches         |\n",
        "| **Risk**               | May underfit if stopped too early | Computationally more expensive         |\n",
        "| **Example parameter**  | `max_depth`, `min_samples_split`  | Cost-complexity pruning (alpha tuning) |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "- **Pre-Pruning** avoids overfitting by **limiting the growth** of the tree in advance, saving time and resources.  \n",
        "- **Post-Pruning** allows the tree to fully explore data patterns and then **removes overfitted branches**, improving generalization.  \n",
        "Both methods are essential in practice, and the choice depends on the trade-off between **efficiency** (pre-pruning) and **accuracy** (post-pruning).  \n",
        "\n",
        "## Q4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?  \n",
        "\n",
        "**Answer:**  -When constructing a Decision Tree, the main goal is to create splits that **best separate the data into homogeneous groups**. One of the most widely used criteria for selecting splits is **Information Gain (IG)**, which is derived from the concept of **Entropy** in information theory.  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Entropy as a Measure of Impurity  \n",
        "Entropy measures the amount of **disorder or uncertainty** in a dataset. For a node with $C$ classes and probabilities $p_i$ of each class $i$, entropy is:  \n",
        "\n",
        "$$\n",
        "Entropy(S) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\n",
        "$$  \n",
        "\n",
        "- If all samples belong to one class, $Entropy = 0$ (pure node).  \n",
        "- If classes are equally distributed, entropy is maximum (highest impurity).  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Information Gain (IG)  \n",
        "**Definition:** Information Gain is the **reduction in entropy** achieved after splitting a dataset based on a particular attribute.  \n",
        "\n",
        "**Formula:**  \n",
        "$$\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot Entropy(S_v)\n",
        "$$  \n",
        "\n",
        "Where:  \n",
        "- $S$ = parent dataset  \n",
        "- $A$ = attribute used for splitting  \n",
        "- $S_v$ = subset of $S$ for which attribute $A$ has value $v$  \n",
        "- $|S_v|/|S|$ = proportion of samples in subset  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why It Matters for Splitting  \n",
        "- At each step, the Decision Tree algorithm evaluates all features.  \n",
        "- The feature that gives the **highest Information Gain** is chosen for the split, because it provides the **greatest reduction in uncertainty**.  \n",
        "- This ensures that child nodes are **purer** than the parent node.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Numerical Example  \n",
        "Suppose we want to predict whether students will **Pass** or **Fail** based on *Study Hours*.  \n",
        "\n",
        "- Parent node: 10 samples → 6 Pass, 4 Fail.  \n",
        "Entropy:  \n",
        "$$\n",
        "Entropy(parent) = - (0.6 \\log_2 0.6 + 0.4 \\log_2 0.4) \\approx 0.97\n",
        "$$  \n",
        "\n",
        "- After split:  \n",
        "  - Node A (5 samples): 4 Pass, 1 Fail → $Entropy(A) \\approx 0.72$  \n",
        "  - Node B (5 samples): 2 Pass, 3 Fail → $Entropy(B) \\approx 0.97$  \n",
        "\n",
        "Weighted average entropy:  \n",
        "$$\n",
        "Entropy_{children} = \\frac{5}{10}(0.72) + \\frac{5}{10}(0.97) = 0.845\n",
        "$$  \n",
        "\n",
        "Information Gain:  \n",
        "$$\n",
        "IG = 0.97 - 0.845 = 0.125\n",
        "$$  \n",
        "\n",
        "This means splitting on *Study Hours* reduces impurity, making it a good candidate feature.  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Importance of Information Gain  \n",
        "- Ensures **maximum reduction in impurity** at each split.  \n",
        "- Guides the algorithm toward creating **purer nodes** and more accurate predictions.  \n",
        "- Prevents arbitrary splits by giving a **quantitative criterion** for feature selection.  \n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "Information Gain is central to Decision Trees as it measures the effectiveness of a split in reducing uncertainty. By consistently choosing attributes with the highest Information Gain, the tree builds a hierarchy of decisions that progressively improve classification accuracy and interpretability.  \n",
        "\n",
        "## Q5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?  \n",
        "\n",
        "**Answer:**-Decision Trees are widely used in practice due to their simplicity, interpretability, and ability to handle diverse types of data. They are applied in multiple real-world domains where **rule-based classification or prediction** is required.  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Common Real-World Applications  \n",
        "\n",
        "1. **Finance and Banking**  \n",
        "   - Loan approval: Banks use decision trees to decide whether to approve or reject a loan based on factors such as income, credit history, and employment status.  \n",
        "   - Fraud detection: Classifies transactions as “fraudulent” or “genuine” by analyzing transaction patterns.  \n",
        "\n",
        "2. **Healthcare**  \n",
        "   - Medical diagnosis: Helps in predicting the presence of a disease based on patient symptoms and test results.  \n",
        "   - Risk stratification: Identifies high-risk patients for preventive care.  \n",
        "\n",
        "3. **Marketing and Customer Analytics**  \n",
        "   - Customer segmentation: Identifies which customers are more likely to respond to promotions.  \n",
        "   - Churn prediction: Classifies customers as “likely to churn” or “retain” based on service usage patterns.  \n",
        "\n",
        "4. **Education**  \n",
        "   - Predicting student performance based on attendance, study hours, and assignment completion.  \n",
        "   - Identifying at-risk students for intervention.  \n",
        "\n",
        "5. **Operations and Manufacturing**  \n",
        "   - Quality control: Classifies defective vs. non-defective items in production.  \n",
        "   - Supply chain optimization: Decision rules for choosing suppliers or logistics routes.  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Advantages of Decision Trees  \n",
        "- **Interpretability:** Easy to understand and explain (tree rules resemble human reasoning).  \n",
        "- **No data scaling required:** Works with raw data without normalization.  \n",
        "- **Handles mixed data types:** Can process both categorical and numerical variables.  \n",
        "- **Non-parametric:** Makes no assumption about data distribution.  \n",
        "- **Feature selection built-in:** Automatically selects the most important attributes for splitting.  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Limitations of Decision Trees  \n",
        "- **Overfitting:** Trees can grow very deep and capture noise in the data.  \n",
        "- **Instability:** Small changes in data can lead to very different tree structures.  \n",
        "- **Bias toward features with many categories:** Attributes with many distinct values can dominate splits.  \n",
        "- **Lower predictive accuracy (alone):** Often less accurate compared to ensemble methods (e.g., Random Forest, Gradient Boosting).  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Practical Example  \n",
        "In banking, a decision tree for loan approval may start with:  \n",
        "- Root node: `Credit Score > 650?`  \n",
        "  - If **Yes → Check Income > 40,000?**  \n",
        "    - If **Yes → Approve Loan**  \n",
        "    - If **No → Further check Employment Stability**  \n",
        "  - If **No → Reject Loan**  \n",
        "\n",
        "This rule-based flow is transparent and explainable to both customers and loan officers.  \n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "Decision Trees have become essential in many domains such as finance, healthcare, marketing, and education because of their clarity and ease of interpretation. Their main **advantage** lies in their **simplicity and transparency**, while their main **limitation** is a tendency to **overfit and instability**. To overcome these drawbacks, advanced ensemble methods like **Random Forests** and **Gradient Boosted Trees** are often used in practice.\n",
        "\n",
        "Dataset Info:\n",
        "-  Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "-  Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        " ## Q6: Write a Python program to:\n",
        "-  Load the Iris Dataset\n",
        "-  Train a Decision Tree Classifier using the Gini criterion\n",
        "-  Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**- Below is the Python program that:\n",
        "1. Loads the **Iris dataset**  \n",
        "2. Trains a **Decision Tree Classifier** using the **Gini criterion**  \n",
        "3. Prints the **accuracy** and **feature importances**  \n",
        "\n"
      ],
      "metadata": {
        "id": "BYR1xeEg4bJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy and feature importances\n",
        "print(\"Decision Tree Classifier (Gini) Results:\")\n",
        "print(\"Accuracy on Test Set:\", round(accuracy, 3))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "print(\"Feature Names:\", iris.feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pSPy1U98wEU",
        "outputId": "cc193db3-b1e8-4bff-ddcc-64946fb56eea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier (Gini) Results:\n",
            "Accuracy on Test Set: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n",
            "Feature Names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output (Actual from running the code):\n",
        "\n",
        "Decision Tree Classifier (Gini) Results:  \n",
        "Accuracy on Test Set: **1.0**  \n",
        "Feature Importances: **[0.0, 0.01667014, 0.90614339, 0.07718647]**  \n",
        "Feature Names: **['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']**  \n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation:\n",
        "- The model achieved **100% accuracy** on the test set with `random_state=42`.  \n",
        "- The most important feature for classification is **petal length** (≈90.6%), followed by **petal width** (≈7.7%).  \n",
        "- Sepal features contribute very little to the classification in this dataset, highlighting that **petal measurements dominate species differentiation**.  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "## Q7: Write a Python program to\n",
        "- Load the Iris Dataset\n",
        "- Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:** - This program:\n",
        "1. Loads the **Iris dataset**  \n",
        "2. Trains a **Decision Tree Classifier** with `max_depth=3`  \n",
        "3. Trains a **fully-grown Decision Tree** for comparison  \n",
        "4. Prints **accuracy scores** of both models and compares them.\n"
      ],
      "metadata": {
        "id": "tiLgrNyp9C0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, criterion='gini', random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Initialize fully-grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree with max_depth=3 Accuracy:\", round(accuracy_limited, 3))\n",
        "print(\"Fully-grown Decision Tree Accuracy:\", round(accuracy_full, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUeGKebFBWa7",
        "outputId": "48c3f075-953d-4494-e6cd-3a3e81f53a03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree with max_depth=3 Accuracy: 1.0\n",
            "Fully-grown Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output (may vary slightly with random split):\n",
        "\n",
        "Decision Tree with max_depth=3 Accuracy: 1.0  \n",
        "Fully-grown Decision Tree Accuracy: 1.0  \n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation:\n",
        "- The **limited-depth tree (max_depth=3)** achieves the **same accuracy** as the fully-grown tree on this dataset.  \n",
        "- Limiting depth helps **reduce overfitting**, makes the tree **simpler and more interpretable**, and often performs similarly on small, well-structured datasets like Iris.  \n",
        "- Fully-grown trees can be deeper, potentially memorizing training data, which may reduce generalization in larger or noisier datasets.  \n",
        "\n",
        "## Q8: Write a Python program to:\n",
        "- Load the California Housing dataset from sklearn\n",
        "-  Train a Decision Tree Regressor\n",
        "-  Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:** -This program:\n",
        "1. Loads the **California Housing dataset** from `sklearn.datasets`  \n",
        "2. Trains a **Decision Tree Regressor**  \n",
        "3. Prints the **Mean Squared Error (MSE)** and **feature importances**\n"
      ],
      "metadata": {
        "id": "0d-qf2MYBdp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X = california.data\n",
        "y = california.target\n",
        "feature_names = california.feature_names\n",
        "\n",
        "# Split dataset into training and testing sets (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Decision Tree Regressor Results:\")\n",
        "print(\"Mean Squared Error (MSE) on Test Set:\", round(mse, 3))\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n",
        "print(\"Feature Names:\", feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf6b4pKbETLQ",
        "outputId": "19acc193-c1c6-4169-c943-9e8c55f81dc0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor Results:\n",
            "Mean Squared Error (MSE) on Test Set: 0.495\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n",
            "Feature Names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output:\n",
        "\n",
        "Decision Tree Regressor Results:  \n",
        "Mean Squared Error (MSE) on Test Set: 0.495  \n",
        "Feature Importances: [0.139, 0.0, 0.003, 0.0, 0.02, 0.77, 0.005, 0.063]  \n",
        "Feature Names: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']  \n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation:\n",
        "- The **MSE ≈ 0.495** indicates the average squared error between predicted and actual median house values.  \n",
        "- The most influential feature is **AveOccup** (average occupancy per house), followed by **MedInc** (median income), which aligns with expectations that income and household size strongly impact housing prices.  \n",
        "- Features like `HouseAge` and `AveBedrms` have minimal impact on the model’s predictions.  \n",
        "\n",
        "## Q9: Write a Python program to:\n",
        "-  Load the Iris Dataset\n",
        "-  Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "-  Print the best parameters and the resulting model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "**Answer:**  -This program:\n",
        "1. Loads the **Iris dataset**  \n",
        "2. Uses **GridSearchCV** to tune `max_depth` and `min_samples_split` of a Decision Tree Classifier  \n",
        "3. Prints the **best hyperparameters** and the corresponding **model accuracy**\n",
        "\n"
      ],
      "metadata": {
        "id": "TEb99ovzEb4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and corresponding model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Test Set Accuracy with Best Model:\", round(accuracy, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wynvIKxYF-8j",
        "outputId": "233af133-634f-43aa-8bb6-d03c10bc9154"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Set Accuracy with Best Model: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output (typical):\n",
        "\n",
        "Best Hyperparameters: {'max_depth': 3, 'min_samples_split': 2}  \n",
        "Test Set Accuracy with Best Model: 1.0  \n",
        "\n",
        "---\n",
        "\n",
        "### Interpretation:\n",
        "- The **best hyperparameters** found by GridSearchCV are `max_depth=3` and `min_samples_split=2`.  \n",
        "- With these parameters, the Decision Tree achieves **100% accuracy** on the test set.  \n",
        "- Hyperparameter tuning helps **control overfitting** by restricting tree depth and minimum split size, improving generalization.  \n",
        "- This demonstrates that even a **shallow tree** can perform optimally on the Iris dataset while remaining interpretable.  \n",
        "\n",
        "\n",
        "##Q 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "-  Handle the missing values\n",
        "- Encode the categorical features\n",
        "-  Train a Decision Tree model\n",
        "- Tune its hyperparameters\n",
        "- Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "**Answer:**  -In a healthcare scenario, building a predictive model requires careful **data preprocessing, model training, hyperparameter tuning, evaluation, and alignment with business goals**.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Handle Missing Values\n",
        "- Identify missing values using `df.isnull().sum()`.  \n",
        "- Impute missing values:  \n",
        "  - Numerical features → **median**  \n",
        "  - Categorical features → **most frequent value**  \n",
        "\n",
        "### Step 2: Encode Categorical Features\n",
        "- Decision Trees require numerical input.  \n",
        "- Categorical features are **one-hot encoded** after imputation.  \n",
        "\n",
        "### Step 3: Train a Decision Tree Model\n",
        "- Split data into **training and testing sets**.  \n",
        "- Train a **Decision Tree Classifier** using the Gini criterion.  \n",
        "\n",
        "### Step 4: Tune Hyperparameters\n",
        "- Use **GridSearchCV** to find the best `max_depth` and `min_samples_split`.  \n",
        "\n",
        "### Step 5: Evaluate Model Performance\n",
        "- Use **classification report** to check Accuracy, Precision, Recall, and F1-score.  \n",
        "- In healthcare, **high recall** is critical to minimize **false negatives**.  \n",
        "\n",
        "### Step 6: Business Value\n",
        "- **Early Detection:** Enables timely intervention, reducing complications and costs.  \n",
        "- **Resource Allocation:** Helps identify high-risk patients and optimize medical resources.  \n",
        "- **Personalized Care:** Supports tailored treatment plans.  \n",
        "- **Data-Driven Decisions:** Assists in planning preventive programs and improving healthcare quality.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UDLslT_xGOvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# No categorical columns in Iris; just numerical\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', 'passthrough', X.columns)\n",
        "])\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [2, 3, None],\n",
        "    'classifier__min_samples_split': [2, 3, 4]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iisHBpfP8Mt",
        "outputId": "61f979d3-6f37-4dc6-ba1c-b9c32704a16d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'classifier__max_depth': 3, 'classifier__min_samples_split': 2}\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        50\n",
            "           1       0.98      0.94      0.96        50\n",
            "           2       0.94      0.98      0.96        50\n",
            "\n",
            "    accuracy                           0.97       150\n",
            "   macro avg       0.97      0.97      0.97       150\n",
            "weighted avg       0.97      0.97      0.97       150\n",
            "\n"
          ]
        }
      ]
    }
  ]
}